[
  {
    "objectID": "people/robin.html",
    "href": "people/robin.html",
    "title": "Robin Long",
    "section": "",
    "text": "Robin is interested in reproducible research, and encouraging and supporting its use in creating sustainable software. Robin’s background began in experimental particle physics supporting high performance computing"
  },
  {
    "objectID": "people/matt_c.html",
    "href": "people/matt_c.html",
    "title": "Matthew Coole",
    "section": "",
    "text": "Matt works to find novel & effective research solutions for projects and helps to encourage software engineering best practices. His background is in large-scale text processing and NLP, & experience applying machine learning and visualisation techniques."
  },
  {
    "objectID": "people/jo.html",
    "href": "people/jo.html",
    "title": "Jo Walsh",
    "section": "",
    "text": "Jo works to find novel and effective research solutions for projects and helping to encourage software engineering best practices. Jo has a lot of background in open source geospatial and semantic web"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "You can contact us by email at rsegroup@ceh.ac.uk — this is preferred over contacting individual team members.\nWe will aim to get back to you within X working days.\n\nFeel free to drop into our discussions on GitHub.\n\nWork with us\nInitial enquiries about working with RSEs should be made to EDSsupport@ceh.ac.uk, or you can fill out the Digital Research enquiry form.\nThe criteria for allocating RSEs are based on the boldness and ambition of the science, its alignment with Strategy 2030, the furtherance of DRI ambitions, the potential to promote key principles, and the availability of resources to cover the costs of RSEs.\nCommunication is key. If you don’t feel your project meets the criteria mentioned above, please still contact us. We may be able to explore avenues to pull different pieces of work from multiple projects that are looking at the same problem. If you are early in the proposal stage, we may be able to help enhance your proposal so that it aligns with the wider UKCEH digital strategy.\n\n\n\n\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "updates/2025-09-11_rsecon/index.html",
    "href": "updates/2025-09-11_rsecon/index.html",
    "title": "RSECon 2025",
    "section": "",
    "text": "ReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "updates/2025-08-30_welcome/index.html",
    "href": "updates/2025-08-30_welcome/index.html",
    "title": "We made a website",
    "section": "",
    "text": "Hello world\nWe made a website.\nWhat you should expect to find:\n\nUpdates, news etc.\nProjects\nTutorials\n\n\n\nWhy we felt we needed a website\n\nGrowing portfolio of projects and other activities\nBut it’s hard to get visibility in UKCEH - big organisation and everyone is busy!\nNeed an easy way to share updates, content\nNeed a place to collect our training materials\nNeed a centralised space so we can just send people a link.\nNeeds to be accessible outside of our org - external collaborators.\n\n\n\nWhy Quarto?\n\nPlain text (markdown) based.\nUsed by many in UKCEH\nExisting UKCEH brand, implemented by Joe.\nCan render executable notebooks in Python/R/Julia - use the same tool for tutorials.\n\n\n\n\n\n\n\nTip\n\n\n\nFor instructions on how to apply the UKCEH branding to your own Quarto documents, see HERE.\n\n\n\n\nWhy not use the Hub?\n\nPrefer a solution based on plain text files that can be version-controlled.\nPrefer open-source and open-access - sometimes not clear if a hub page is visible internally or externally.\nHub doesn’t have the best search functionality, and dates are often missing.\nWe prefer to be in control - we are control freaks.\n\n\n\n\nAuthors\n\n\n\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Interactive plots with Jupyter and ipywidgets\n\n\nThis tutorial demonstrates how to create interactive plots in Jupyter notebooks. This is especially useful for workshops.\n\n\n\n\n\nAugust 29, 2025\n\n\nJoe Marsh Rossney\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create UKCEH-branded documents in Quarto/Shiny\n\n\n\nQuarto\n\nShiny\n\n\n\nThis tutorials introduces Posit’s brand-yml theme configuration, and demonstrates how to apply UKCEH branding (fonts, colours and logos) to Quarto documents (such as this website) and Shiny apps.\n\n\n\n\n\nAugust 31, 2025\n\n\nJoe Marsh Rossney\n\n\n\n\n\nNo matching items\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nSustainable DRI\n\n\nSustainable Digital Research Infrastructure for Environmental Science\n\n\n\nActive\n\nNCDR\n\n\n\n\nJoe Marsh Rossney\n\n\nAugust 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBioDT\n\n\nDevelopment of the BioDT Recreational Potential Model for Scotland\n\n\n\nBLU\n\nBiodiversity & Land Use\n\n\n\n\nJoe Marsh Rossney\n\n\nJune 30, 2025\n\n\n\n\n\n\nNo matching items\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "projects/2025_biodt/index.html",
    "href": "projects/2025_biodt/index.html",
    "title": "BioDT",
    "section": "",
    "text": "Something something\n(Dick et al., 2022) - hard/soft personas\n(Rolph et al., 2024) - describe “digital twin” combining Recreational Potential component and biodiversity component.\n(Marsh Rossney, Tigli, Andrews, Rolph, et al., 2025) - code\n(Marsh Rossney, Tigli, Andrews, Dick, et al., 2025) - reports\n(Tigli et al., 2025) - computed outputs for all of Scotland"
  },
  {
    "objectID": "projects/2025_biodt/index.html#biodt",
    "href": "projects/2025_biodt/index.html#biodt",
    "title": "BioDT",
    "section": "",
    "text": "Something something\n(Dick et al., 2022) - hard/soft personas\n(Rolph et al., 2024) - describe “digital twin” combining Recreational Potential component and biodiversity component.\n(Marsh Rossney, Tigli, Andrews, Rolph, et al., 2025) - code\n(Marsh Rossney, Tigli, Andrews, Dick, et al., 2025) - reports\n(Tigli et al., 2025) - computed outputs for all of Scotland"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UKCEH Research Software Engineers",
    "section": "",
    "text": "The RSE group at UKCEH was founded in 2024 by Professor Gordon Blair and Dr Faiza Samreen, who recruited six RSEs with diverse backgrounds in research and industry.\nEach member of the group brings a unique blend of skills and interests, underpinned by both research and software engineering expertise.\nAs of the 2025 restructure, the RSE team forms part of the Digital Innovation group within the broader National Capability and Digital Research (NCDR) science area.\n\n\n\n\n\n\nThe RSE team in Lancaster, September 2025."
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "UKCEH Research Software Engineers",
    "section": "Latest Updates",
    "text": "Latest Updates\n\n\n\n\n\n\n\n\n\n\nRSECon 2025\n\n\nThree of us attended the annual Research Software Engineering conference.\n\n\n\n\n\nSeptember 11, 2025\n\n\nJoe Marsh Rossney, Matt Dalle Piagge, Robin Long\n\n\n\n\n\n\n\n\n\n\n\n\nICCS Machine Learning Coupling Workshop\n\n\nJoe attended a two-day workshop on hybrid Machine Learning x numerical models, run by the Institute of Computing for Climate Science at the…\n\n\n\n\n\nSeptember 4, 2025\n\n\nJoe Marsh Rossney\n\n\n\n\n\n\n\n\n\n\n\n\nWe made a website\n\n\nThis is the first post for this page. I describe some things.\n\n\n\n\n\nAugust 30, 2025\n\n\nJoe Marsh Rossney\n\n\n\n\n\nNo matching items\n\nMore updates…"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "UKCEH Research Software Engineers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince 2025, UKCEH has five science areas:\n\nBiodiversity and Land Use\nEnvironmental Pressures and Responses\nLand-Atmosphere Interactions\nNational Capability & Digital Research\nWater and Climate Science\n\n↩︎\nThe four ‘pillars’ of research software engineering:\n\n\n\n\n\n\nFigure 1: From J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, “The Four Pillars of Research Software Engineering,” in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, 10.1109/MS.2020.2973362.\n\n\n\n↩︎\nFAIR stands for\n\nFindable\nAccessible\nInteroperable\nReusable\n\nThe FAIR principles for research software (FAIR4RS) develop this taxonomy.\nChue Hong, N. P. et al. RDA FAIR4RS WG. (2022). FAIR Principles for Research Software (FAIR4RS Principles) (1.0). Zenodo. 10.15497/RDA00068↩︎"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Members",
    "section": "",
    "text": "Jo Walsh\n\n\n\nSenior RSE\n\nEdinburgh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoe Marsh Rossney\n\n\n\nRSE\n\nEdinburgh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatt Dalle Piagge\n\n\n\nRSE\n\nWallingford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatthew Coole\n\n\n\nSenior RSE\n\nLancaster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobin Long\n\n\n\nSenior RSE\n\nLancaster\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2025_sdri/index.html",
    "href": "projects/2025_sdri/index.html",
    "title": "Sustainable DRI",
    "section": "",
    "text": "Note\n\n\n\nThis is an active project.\n\n\n\n\n\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "tutorials/brand-yml/index.html",
    "href": "tutorials/brand-yml/index.html",
    "title": "How to create UKCEH-branded documents in Quarto/Shiny",
    "section": "",
    "text": "brand.yml is Posit’s attempt to develop a unified mechanism for theme configuration — logos, colours, fonts and typography — across different output formats — documents, presentations, websites, dashboards, apps — produced by Quarto or Shiny projects.\n\n\n\n\n\nbrand.yml provides a way to single-source this configuration as a plain-text YAML file. This is particularly useful for organisations such as UKCEH since the configuration can be distributed from a single, centrally managed repository on GitHub.\nA GitHub-hosted brand.yml can be applied to existing projects without any modifications to the source, by ‘installing’ it as an extension. As a bonus, any changes to the theme (e.g. due to a rebranding) can be applied retroactively by simply updating the extension (essentially git pull).\n\n\nVisit ukceh-rse.github.io/brand-yml to see some examples of the UKCEH brand applied to some basic Quarto documents (HTML page, pdf and presentation)."
  },
  {
    "objectID": "tutorials/brand-yml/index.html#introduction",
    "href": "tutorials/brand-yml/index.html#introduction",
    "title": "How to create UKCEH-branded documents in Quarto/Shiny",
    "section": "",
    "text": "brand.yml is Posit’s attempt to develop a unified mechanism for theme configuration — logos, colours, fonts and typography — across different output formats — documents, presentations, websites, dashboards, apps — produced by Quarto or Shiny projects.\n\n\n\n\n\nbrand.yml provides a way to single-source this configuration as a plain-text YAML file. This is particularly useful for organisations such as UKCEH since the configuration can be distributed from a single, centrally managed repository on GitHub.\nA GitHub-hosted brand.yml can be applied to existing projects without any modifications to the source, by ‘installing’ it as an extension. As a bonus, any changes to the theme (e.g. due to a rebranding) can be applied retroactively by simply updating the extension (essentially git pull).\n\n\nVisit ukceh-rse.github.io/brand-yml to see some examples of the UKCEH brand applied to some basic Quarto documents (HTML page, pdf and presentation)."
  },
  {
    "objectID": "tutorials/brand-yml/index.html#how-to-use-brand-yml",
    "href": "tutorials/brand-yml/index.html#how-to-use-brand-yml",
    "title": "How to create UKCEH-branded documents in Quarto/Shiny",
    "section": "2 How to use brand-yml",
    "text": "2 How to use brand-yml\n\n\n\n\n\n\nWarning\n\n\n\nThis will only work with Quarto version 1.8 or higher! As of 20/08/2025 this is still in pre-release, but it can be downloaded here.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis will only work for projects, i.e. when there is a _quarto.yml file containing the project field.\nHowever, there is no real downside to upgrading stand-alone documents to projects. Simply create a _quarto.yml file with the following contents:\nproject:\n  type: default\n\n\n\n2.1 Existing projects\nFirst, install the extension.\n\nCommand lineRStudio\n\n\nRun the following command in the root directory of your quarto project.\nquarto add ukceh-rse/brand-yml\n\n\nTo do.\n\n\n\nThis will install the extension under the _extensions subdirectory. If you’re using version control, you will want to check in this directory.\nYou will need to specify the desired format when you render.\n\nCommand lineRStudio\n\n\nquarto render --to FORMAT\n\n\nTo do\n\n\n\nIf you plan to render to multiple formats with the same file extension you will also need to specify an output file (--output FILE) so that they don’t overwrite each other.\n\n\n2.2 New projects\nThe extension also serves as a starter template for stand-alone Quarto documents and presentations. There are currently 5 formats provided out of the box.\n\nhtml\ntypst\ntypst-dark\nrevealjs\nrevealjs-dark\n\n\nCommand lineRStudio\n\n\nTo create a new project based on this template, run the following:\nquarto use template ukceh-rse/brand-yml\n\n\nTo do.\n\n\n\n\n\n2.3 Without installing an extension\nAn alternative is to simply download the contents of _extensions/brand/ and manually place them in your quarto project.\nRename brand.yml -&gt; _brand.yml so that it is picked up automatically. This will also work for non-projects (no _quarto.yml)."
  },
  {
    "objectID": "tutorials/brand-yml/index.html#faq",
    "href": "tutorials/brand-yml/index.html#faq",
    "title": "How to create UKCEH-branded documents in Quarto/Shiny",
    "section": "3 FAQ",
    "text": "3 FAQ\n\n3.1 Why not use the existing templates?\nWe have a couple of very nice templates for UKCEH-branded powerpoint presentations and word documents.\nHowever, people increasingly prefer to work with plain text documents, which are superior to Microsoft’s formats in a number of ways:\n\nPortability: can be read and edited on any device without requiring proprietary or closed-source software\nLongevity: you will be able to open, read and edit plain text files in 20 years time. Can you say the same for a Word document?\nCompatibility with version control software.\nSupports multiple output formats from a single plain text source.\nSupports blending of executable code and regular text.\n\nPerhaps most importantly, a significant number of people with UKCEH already use Quarto, RMarkdown (its predeccessor) and Shiny already. There are already numerous official UKCEH outputs — mostly dashboards and Shiny apps — that were produced with these tools, but with the branding applied loosely or not at all.\n\n\n3.2 How is this consistent with the UKCEH branding?\nThe logos, colours, fonts and typographic information used in brand-yml were lifted from the UKCEH Brand Book (v4.0, September 2024).\nThe brand book does not provide guidance on dark-themed documents, so I applied a little creative freedom here in deciding which colours to use as primary, secondary, background etc.\n\n\n3.3 Has this been approved by Comms?\nShort answer — no.\nLonger answer — not yet. I am in contact with folk in the design team and they are on the whole positive about the whole thing. However, they have wisely suggested waiting until the ongoing rebrand has sorted itself out before pushing ahead with efforts to make this ‘official’.\n\n\n3.4 Where is the UKCEH brand-yml kept?\nOn GitHub, at github.com/ukceh-rse/brand-yml.\nThe repository is currently housed under the RSE’s own GitHub organisation (github.com/ukceh-rse). Once (if) this gets approved by the Comms team for organisation-wide use, we may find that it needs to be moved to the official UKCEH GitHub organisation (github.com/NERC-CEH).\n\n\n3.5 What about templates that reproduce the official Word and Powerpoint templates?\nThis has been on my radar for a while, and I made a start with a Quarto/RevealJS presentation template that closely aligns with the official Powerpoint template.\nHowever, it involves a fair amount of Javascript to get working, and I’m not overly satisfied with my initial attempt.\nI plan to revisit this once the rebranding is complete, which will likely result in the official document templates changing anyway.\n\n\n3.6 I’ve found a bug, what do I do?\nPlease raise an issue on GitHub if you are able to. Otherwise get in touch with Joe directly."
  },
  {
    "objectID": "tutorials/interactive-plots/index.html",
    "href": "tutorials/interactive-plots/index.html",
    "title": "Interactive plots with Jupyter and ipywidgets",
    "section": "",
    "text": "import logging\n\nfrom rich.logging import RichHandler\n\nlogging.basicConfig(\n    level=\"INFO\",\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n\nlogger = logging.getLogger()\n\n\nfrom typing import Callable\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n# For easy reading\nπ = np.pi\n\n\n%matplotlib widget\n\n\n# Define the sympy symbol\nx = sympy.symbols(\"x\")\n\n# A subset of SymPy functions that are (mostly) differentiable\nALLOWED_FUNCS = {\n    \"x\": x,\n    \"sin\": sympy.sin,\n    \"cos\": sympy.cos,\n    \"tan\": sympy.tan,\n    \"asin\": sympy.asin,\n    \"acos\": sympy.acos,\n    \"atan\": sympy.atan,\n    \"sinh\": sympy.sinh,\n    \"cosh\": sympy.cosh,\n    \"tanh\": sympy.tanh,\n    \"exp\": sympy.exp,\n    \"log\": sympy.log,\n    \"ln\": sympy.log,\n    \"sqrt\": sympy.sqrt,\n    \"pi\": sympy.pi,\n    \"π\": sympy.pi,\n}\n\n\ndef parse(expression_str: str) -&gt; tuple[sympy.Expr, sympy.Expr]:\n    \"\"\"Parse a string into a sympy expression f(x) and its derivative f'(x).\"\"\"\n    try:\n        f = sympy.sympify(expression_str.strip(), locals=ALLOWED_FUNCS)\n    except Exception as e:\n        raise ValueError(f\"Could not parse expression: {e}\")\n\n    # Ensure it's a function of x only\n    free_syms = f.free_symbols\n    if not free_syms == {x}:\n        raise ValueError(\"Expression must be a function of x only.\")\n\n    # Reject any undefined function applications\n    undefined = f.atoms(sympy.core.function.AppliedUndef)\n    if undefined:\n        names = \", \".join(sorted(str(u.func) for u in undefined))\n        allowed = \", \".join(\n            sorted(k for k in ALLOWED_FUNCS if callable(ALLOWED_FUNCS[k]))\n        )\n        raise ValueError(f\"Unknown function(s): {names}. Allowed: {allowed}\")\n\n    dfdx = sympy.diff(f, x)\n\n    return f, dfdx\n\n\ndef lambdify(expression: sympy.Expr) -&gt; Callable[[np.ndarray], [np.ndarray]]:\n    return sympy.lambdify(x, expression, modules=[\"numpy\"])\n\n\ndef safe_eval(func, xs):\n    \"\"\"\n    Evaluate a numpy-lambdified function with error handling.\n    Returns an array with non-finite values masked to np.nan.\n    \"\"\"\n    with np.errstate(all=\"ignore\"):\n        return func(xs)\n\n    \"\"\"\n        y = np.array(func(xs), dtype=float)\n    # Ensure shape compatibility\n    if y.shape != xs.shape:\n        try:\n            y = np.broadcast_to(y, xs.shape)\n        except Exception:\n            y = np.full_like(xs, np.nan, dtype=float)\n    # mask non-finite\n    y[~np.isfinite(y)] = np.nan\n    return y\n    \"\"\"\n\n\nf_expr, dfdx_expr = parse(\"sin(x)\")\nf, dfdx = map(lambdify, [f_expr, dfdx_expr])\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\n# NOTE: we cannot use `x` since this is reserved by sympy!\nX = np.linspace(-2 * π, 2 * π, 100)\nax.set_xlim(-2 * π, 2 * π)\n\nf, dfdx = map(lambdify, parse(\"sin(x)\"))\n\nax.plot(X, f(X), label=f\"f(x)={f_expr}\")\nax.plot(X, dfdx(X), label=f\"f'(x)={dfdx_expr}\")\nax.legend()\n\n\n\n\n\ndef plot_with_slider():\n    fig, ax = plt.subplots(figsize=(6.5, 4), constrained_layout=True)\n\n    def update(expression_str: str, domain: tuple[float, float], x0: float, N: int):\n        # Parse expression\n        try:\n            f_expr, dfdx_expr = parse(expression_str)\n        except Exception as e:\n            logging.error(e)\n            return\n\n        # Check valid domain\n        xmin, xmax = domain\n        if xmin &gt;= xmax:\n            logging.error(\"`xmin` must be strictly less than `xmax`\")\n            return\n\n        logging.info(f\"Valid expression: {f_expr}\")\n\n        # Clear the canvas\n        # NOTE: this is a lazy way to do it - it would be better to remove elements individually\n        # The downside of being lazy is we have to re-draw everything, including the title\n        plt.cla()\n\n        ax.set_title(\"Functions and their tangents\")\n        ax.set_xlabel(\"$x$\")\n        ax.set_ylabel(\"$y$\")\n        ax.set_xlim(xmin, xmax)\n\n        # Lambdify\n        f, dfdx = map(lambdify, [f_expr, dfdx_expr])\n\n        # Compute and plot function\n        X = np.linspace(xmin, xmax, N)\n        y = safe_eval(f, X)\n        ax.plot(X, y, label=f\"$f(x) = {sympy.latex(f_expr)}$\")\n\n        # Compute and plot tangent\n        # y0 = float(robust_eval(f_np, np.array([x0]))[0])\n        # m = float(robust_eval(df_np, np.array([x0]))[0])\n        y0 = float(safe_eval(f, x0))\n        m = float(safe_eval(dfdx, x0))\n        dx = 0.05 * (xmax - xmin)  # ±5% on either side of x0\n        x1, x2 = x0 - dx, x0 + dx\n        y1 = y0 + m * (x1 - x0)\n        y2 = y0 + m * (x2 - x0)\n        ax.plot(\n            [x1, x2],\n            [y1, y2],\n            color=\"red\",\n            linewidth=2.5,\n            label=rf\"$x_0={x0:+.3f}$\" + \"\\n\" + rf\"$f'(x_0)={m:+.3f}$\",\n        )\n\n        # The point\n        ax.plot([x0], [y0], \"o\", color=\"red\")\n\n        ax.legend(loc=\"upper right\")\n\n    return update\n\n\n_ = interact(\n    plot_with_slider(),\n    expression_str=widgets.Text(\n        value=\"sin(x)\", description=\"f(x):\", layout=widgets.Layout(width=\"400px\")\n    ),\n    domain=widgets.FloatRangeSlider(\n        value=(-2 * π, 2 * π),\n        min=-4 * π,\n        max=4 * π,\n        step=0.01,\n        description=\"Domain\",\n        layout=widgets.Layout(width=\"400px\"),\n    ),\n    x0=widgets.FloatSlider(\n        value=0.0,\n        min=-2 * π,\n        max=2 * π,\n        step=0.01,\n        description=\"x0\",\n        layout=widgets.Layout(width=\"400px\"),\n    ),\n    N=widgets.fixed(800),\n)\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "updates/2025-09-04_ml_coupling_workshop/index.html",
    "href": "updates/2025-09-04_ml_coupling_workshop/index.html",
    "title": "ICCS Machine Learning Coupling Workshop",
    "section": "",
    "text": "Earlier this week I was fortunate enough to attend the Machine Learning Coupling workshop run by the Institute of Computing for Climate Science at the University of Cambridge. The workshop name (“Machine Learning Coupling”) has a slightly odd ring to it, but is referring to the challenge of constructing “hybrid” numerical/statistical models that graft machine learning (ML) components (the statistical, or data-driven part) onto a traditional numerical modelling rootstock.\nMost speakers and attendees were climate modellers to first approximation, although there was a smattering of ‘outsiders’ including plasma physicists, statisticians, mathematical biologists, and me.\n\n\nHybrid modelling has had the aura of a Next Big Thing™ for some years now, because\n\n“machine learning can’t completely replace physics-based modelling, obviously1 — they need to be combined.”\n\n\nI think this is broadly right.2 People like me (but smarter) are interested in this because they are convinced that somewhere in the hybrid modelling space lies a best-of-both-worlds situation, which fully exploits our prior knowledge about the system we’re modelling and augments those priors with new knowledge encoded into statistical models that have been fit to data.\n\n\n\nBuilding complex hybrid models extremely challenging — less like Lego and more like organ transplant surgery.\nThis shouldn’t be surprising really; numerical modelling communities and statistical/machine learning communities established their own separate ecosystems largely independent of one another over the course of decades. Hence, existing models, methodologies and tools were not designed with this sort of coupling in mind.\n\n\nLet’s take a simple example to make this more explicit. Say I train a neural network in PyTorch with the intention of replacing some highly uncertain empirical component of a Fortran model. The neural network seems to achieve a very high prediction accuracy, so I quickly publish a paper to tell everyone about this breakthrough that will result in faster and more accurate model predictions (maybe I even took the time to benchmark it against the original empirical model).\nSo now what? How do I actually ‘put’ the neural network in the Fortran model?\nOne possible approach is to pause the execution of the Fortran model just before the substituted component is called, write the required network inputs to some location in memory, load these inputs into PyTorch and execute the forward pass of the neural network, write the network outputs to memory, read them back into the Fortran model and continue the run. If it’s not obvious why this is a bad approach, you might consider a brief skim of this Wikipedia page.\nI could, instead, manually implement the forward pass of the neural network in Fortran, loading the weights and biases from a CSV file at runtime. This is actually a pretty good option provided I have no intention of making any further changes to the model architecture (e.g. changing any layer sizes or adding another layer) and I’m not interested in “online” training while the model is being run. Oh, and I need to know how to code up a neural network in Fortran.\nOtherwise what I’m really left with is a choice between “some kind of wrapper” that allows me to call PyTorch models directly from Fortran, or rewriting the Fortran model in a more ML friendly language such as Julia.\n\n\n\n\nFortunately the second-last of the above options (“some kind of wrapper”) has been made possible thanks to a significant effort by the folks at ICCS in the form of FTorch (Atkinson et al., 2025). A lot of workshop attendees were using FTorch already.\nThere were three main strategies vocalised by participants at this workshop.\n\n\n\n\n\nflowchart TD\n    A[\"Building hybrid ML/numerical models\"]\n\n    B[\"Implement forward pass in pure Fortran\"]\n    C[\"Couple ML model to Fortran model\"]\n    D[\"Rewrite model in ML-friendly language\"]\n    E[\"FTorch\"]\n    F[\"Jax\"]\n    G[\"Julia\"]\n\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n\n    C --&gt; E\n    D --&gt; F\n    D --&gt; G\n\n\n\n\n\n\nThere was a brief mention of a Fortran-native automatic differentiation (AD) library, but it sounded like a case of “there’s this one guy trying to do it.”\nUp to now I haven’t even mentioned CPU–GPU data transfer. This can be a significant bottleneck, frequently mitigating any performance gains from executing the neural network on the GPU. Projects like Oceananigans (Silvestri et al., 2023) — the “spiritual successor” to MITgcm — go much further than translating the code from Fortran to Julia; they completely overhaul the design so that the entire model, not just the ML components, run efficiently on GPUs.\n\n\n\nThe Mathematical Bridge over the river Cam."
  },
  {
    "objectID": "updates/2025-09-04_ml_coupling_workshop/index.html#introduction",
    "href": "updates/2025-09-04_ml_coupling_workshop/index.html#introduction",
    "title": "ICCS Machine Learning Coupling Workshop",
    "section": "",
    "text": "Earlier this week I was fortunate enough to attend the Machine Learning Coupling workshop run by the Institute of Computing for Climate Science at the University of Cambridge. The workshop name (“Machine Learning Coupling”) has a slightly odd ring to it, but is referring to the challenge of constructing “hybrid” numerical/statistical models that graft machine learning (ML) components (the statistical, or data-driven part) onto a traditional numerical modelling rootstock.\nMost speakers and attendees were climate modellers to first approximation, although there was a smattering of ‘outsiders’ including plasma physicists, statisticians, mathematical biologists, and me.\n\n\nHybrid modelling has had the aura of a Next Big Thing™ for some years now, because\n\n“machine learning can’t completely replace physics-based modelling, obviously1 — they need to be combined.”\n\n\nI think this is broadly right.2 People like me (but smarter) are interested in this because they are convinced that somewhere in the hybrid modelling space lies a best-of-both-worlds situation, which fully exploits our prior knowledge about the system we’re modelling and augments those priors with new knowledge encoded into statistical models that have been fit to data.\n\n\n\nBuilding complex hybrid models extremely challenging — less like Lego and more like organ transplant surgery.\nThis shouldn’t be surprising really; numerical modelling communities and statistical/machine learning communities established their own separate ecosystems largely independent of one another over the course of decades. Hence, existing models, methodologies and tools were not designed with this sort of coupling in mind.\n\n\nLet’s take a simple example to make this more explicit. Say I train a neural network in PyTorch with the intention of replacing some highly uncertain empirical component of a Fortran model. The neural network seems to achieve a very high prediction accuracy, so I quickly publish a paper to tell everyone about this breakthrough that will result in faster and more accurate model predictions (maybe I even took the time to benchmark it against the original empirical model).\nSo now what? How do I actually ‘put’ the neural network in the Fortran model?\nOne possible approach is to pause the execution of the Fortran model just before the substituted component is called, write the required network inputs to some location in memory, load these inputs into PyTorch and execute the forward pass of the neural network, write the network outputs to memory, read them back into the Fortran model and continue the run. If it’s not obvious why this is a bad approach, you might consider a brief skim of this Wikipedia page.\nI could, instead, manually implement the forward pass of the neural network in Fortran, loading the weights and biases from a CSV file at runtime. This is actually a pretty good option provided I have no intention of making any further changes to the model architecture (e.g. changing any layer sizes or adding another layer) and I’m not interested in “online” training while the model is being run. Oh, and I need to know how to code up a neural network in Fortran.\nOtherwise what I’m really left with is a choice between “some kind of wrapper” that allows me to call PyTorch models directly from Fortran, or rewriting the Fortran model in a more ML friendly language such as Julia.\n\n\n\n\nFortunately the second-last of the above options (“some kind of wrapper”) has been made possible thanks to a significant effort by the folks at ICCS in the form of FTorch (Atkinson et al., 2025). A lot of workshop attendees were using FTorch already.\nThere were three main strategies vocalised by participants at this workshop.\n\n\n\n\n\nflowchart TD\n    A[\"Building hybrid ML/numerical models\"]\n\n    B[\"Implement forward pass in pure Fortran\"]\n    C[\"Couple ML model to Fortran model\"]\n    D[\"Rewrite model in ML-friendly language\"]\n    E[\"FTorch\"]\n    F[\"Jax\"]\n    G[\"Julia\"]\n\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n\n    C --&gt; E\n    D --&gt; F\n    D --&gt; G\n\n\n\n\n\n\nThere was a brief mention of a Fortran-native automatic differentiation (AD) library, but it sounded like a case of “there’s this one guy trying to do it.”\nUp to now I haven’t even mentioned CPU–GPU data transfer. This can be a significant bottleneck, frequently mitigating any performance gains from executing the neural network on the GPU. Projects like Oceananigans (Silvestri et al., 2023) — the “spiritual successor” to MITgcm — go much further than translating the code from Fortran to Julia; they completely overhaul the design so that the entire model, not just the ML components, run efficiently on GPUs.\n\n\n\nThe Mathematical Bridge over the river Cam."
  },
  {
    "objectID": "updates/2025-09-04_ml_coupling_workshop/index.html#highlights",
    "href": "updates/2025-09-04_ml_coupling_workshop/index.html#highlights",
    "title": "ICCS Machine Learning Coupling Workshop",
    "section": "Highlights",
    "text": "Highlights\n\n#1 - Differentiable programming\nThe talk that I was looking forward to most was one on differentiable programming, jointly delivered by two very impressive scientist/RSEs and Julia advocates, who also happen to be the brains behind the AD library Enzyme3 (Moses & Churavy, 2020) and the modular atmospheric circulation model SpeedyWeather (Klöwer et al., 2024). The AD/Enzyme part of this talk is published online here.\nA wide range of problems can be attacked by optimising a differentiable scalar ‘loss’ function using variations of gradient descent. For modellers, I’m talking about things like sensitivity analysis, parameter calibration and data assimilation, but of course the most well-known example is gradient-based machine learning.\nNot all models are differentiable, even in principle. Many traditional models contain abrupt transitions between different regimes, often generated by control flow (e.g. if-else) statements, at which point the derivative is undefined.4 Others contain embedded stochastic components which might rely on non-differentiable algorithms such as rejection sampling.\nStill, the main barrier is just the software engineering. Some of these models have been slowly built up over decades and are both enormous and enormously complex. It might be feasible to rewrite individual components to make them differentiable, but in many cases components are so deeply intertwined that doing so would trigger a cascade of complex changes throughout the rest of the model.\n\nWhich language for differentiable programming?\nIn a breakout group on the same topic there was a roughly equal three-way split between Julia users, Jax users, and a ‘manual differentiation’ group implementing adjoints and tangents manually in Fortran.\nI’m spiritually in the Julia camp, but for a few reasons will probably stick with Jax for now. The main reason is that I’m far more research than I am software engineer, and too limited in both skill and time to dig myself out of holes caused by undocumented upstream bugs. The other main reason is that most of my colleagues at UKCEH use R, so it’s already asking a lot to persuade them to work with Python, and Julia might be a step too far. In the longer term I think Julia could become my go-to language for modelling problems, if not data problems, and I’m especially looking forward to exploring the CliMA ecosystem.\n\n\n\n#2 - Data-driven discovery of a reduced complexity model for cloud cover\nI liked this talk a lot because it combined the high-level picture with some solid practical advice gained by actually building the thing. The section I want to highlight here is about using machine learning to improve the cloud-cover component of the ICON climate model.\nInitially they went down the classic route of training a vanilla feedforward neural network on some (cloud‑cover) data and transplanting the trained network into the model. It worked ok — and at this point many people would simply say job done, or optimise some hyper-parameters — but they wanted something faster and more interpretable.\nThey replaced the neural network with an analytical ansatz, and used the same cloud cover data to determine the most important terms and their coefficients by symbolic regression. Notably, the procedure revealed an important term that is typically missing from standard parametrisations.\nThis resulted in a model with just a handful of coefficients multiplying terms that have unambiguous physical interpretations, which nevertheless performed on par with the neural network while being much faster to compute. To top things off, they were able to impose physical constraints on the analytical model that would have been much more difficult to achieve with the neural network.\nDetails about this work can be found in Grundner et al. (2023).\n\n\n#3 - GPUs and the future of scientific computing\nI’m paraphrasing a very interesting conversation that I was on the periphery of.\nWe (scientists and publicly funded research organisations) are no longer the primary customers for GPU vendors — as one put it, “we are small fish”. We should not therefore expect the machine learning ecosystem to prioritise our needs. One early warning sign is some new GPU lines de-prioritising or even dropping Float64, which suits data and machine learning workloads but makes them unsuitable for much scientific numerical modelling.\nThese worries will be irrelevant if we cannot afford to buy high-end GPUs in the first place. Given the rapid increase in GPU costs and the potential slowing down of public sector investment, it’s certainly not guaranteed that we will always be able to find the money.\nSo what’s the silver lining?\nWell… GPUs are not a good solution to every computational problem. Porting most existing scientific codes to GPU would probably result in slower execution and massive amounts of wasted time and energy due to under-utilisation of the GPU and expensive data transfer between devices.5\nWe probably need to be much more thoughtful and intentional about which problems demand acceleration that is best served by GPUs (as opposed to other optimisations) and can be reformulated (parallelised), implemented, and scaled so as to effectively saturate the GPU, and minimise waste / interleaves operations efficiently."
  },
  {
    "objectID": "updates/2025-09-04_ml_coupling_workshop/index.html#summary-reflections",
    "href": "updates/2025-09-04_ml_coupling_workshop/index.html#summary-reflections",
    "title": "ICCS Machine Learning Coupling Workshop",
    "section": "Summary & Reflections",
    "text": "Summary & Reflections\nIt is easy for me to see why the Julia folks decided to peel off and start again from a clean slate, using a modern language that makes it easy to build in differentiability, GPU-optimisation and other nice things from the foundations. This is certainly the direction I intend to persue in my work. That said, I have enormous respect for those who choose to dedicate their time to getting more out of existing models. For me, it is clear that both approaches are important and need to happen in tandem.\nWe should acknowledge that the models we rely on now cannot be smoothly updated, and that innovation will be severely constrained if we pour all of our resources into their maintenance. At the same time, there is a huge amount of valuable knowledge embedded in these communities and in the models themselves, which needs to be conserved even as the next generation of models ramp up.\nFrom my (relatively uninformed) perspective, the biggest challenge is to figure out sustainable funding model supports the graceful continuation of existing models and communities while making space for the ‘disruptors’ who want to move far more quickly — that is, give them a genuine alternative to going to work for Google or Nvidia.\n\nOn a final note\nI’ve never been to Cambridge before and it was really quite lovely — especially the old town and the college who put us up — although to me the whole city gave off distinctly ‘school’ vibes. Walking through well-kept grounds to an ancient dining hall,6 I kept expecting to be called to attention by a gowned professor chastising me for not wearing my clothes properly or sufficiently embodying the spirit of Selwyn College.\n\n\n\nSelwyn College, where workshop attendees stayed."
  },
  {
    "objectID": "updates/2025-09-04_ml_coupling_workshop/index.html#footnotes",
    "href": "updates/2025-09-04_ml_coupling_workshop/index.html#footnotes",
    "title": "ICCS Machine Learning Coupling Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt fairness this conclusion is not entirely obvious based on recent press. For example, machine learning-based weather forecasts are currently performing on-par with or better than traditional numerical ensembles for medium-range predictions. However, this is probably a reflection of how poorly traditional models are able to take advantage of observations (data assimilation), rather than because physics priors are not useful.↩︎\nMy own (admittedly rather narrow) experiences with machine learning in physics were a lesson in the judicious use of physical and mathematical priors, which tended to produce smaller, cheaper models that performed far better.↩︎\nEnzyme is pretty rad. AD is performed at the intermediate representation level (MLIR), which means it can work with any LLVM-based language including C, C++, Rust, and even modern Fortran!↩︎\nPoint-like discontinuities (i.e. piecewise differentiable functions) are actually perfectly tolerable, although abrupt changes in gradients can lead to numerical instabilities. But many AD frameworks will just refuse to differentiate through control flow logic.↩︎\nGenerally speaking, small problems (including small ML problems) are faster on CPU unless you can move the entire problem to the GPU (like Oceananigans).↩︎\nWalls lined with portraits of old white dudes staring down at you while you eat - check.↩︎"
  },
  {
    "objectID": "updates/index.html",
    "href": "updates/index.html",
    "title": "Updates",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRSECon 2025\n\n\n\n\n\n\nsoftware\n\ncommunity\n\n\n\nThree of us attended the annual Research Software Engineering conference.\n\n\n\n\n\nSeptember 11, 2025\n\n\nJoe Marsh Rossney, Matt Dalle Piagge, Robin Long\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nICCS Machine Learning Coupling Workshop\n\n\n\n\n\n\nsoftware\n\ncommunity\n\n\n\nJoe attended a two-day workshop on hybrid Machine Learning x numerical models, run by the Institute of Computing for Climate Science at the University of Cambridge.\n\n\n\n\n\nSeptember 4, 2025\n\n\nJoe Marsh Rossney\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nWe made a website\n\n\n\n\n\n\nsoftware\n\ncommunity\n\ntraining\n\npolicy\n\n\n\nThis is the first post for this page. I describe some things.\n\n\n\n\n\nAugust 30, 2025\n\n\nJoe Marsh Rossney\n\n1 min\n\n\n\n\nNo matching items\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "To do\n\n\n\nReuseCC BY 4.0Copyright2025, UK Centre for Ecology & Hydrology Research Software Engineering Group"
  },
  {
    "objectID": "people/joe.html",
    "href": "people/joe.html",
    "title": "Joe Marsh Rossney",
    "section": "",
    "text": "Joe’s academic background spans theoretical physics, machine learning and education research. He is particularly interested in how mathematical models can compliment other forms of scientific inquiry, and support interdisciplinary learning.\n\nBackground\nx\n\n\nProjects\nx"
  },
  {
    "objectID": "people/matt_dp.html",
    "href": "people/matt_dp.html",
    "title": "Matt Dalle Piagge",
    "section": "",
    "text": "Matt is currently investigating object storage as a solution for reproducible and efficient gridded data workflows. His background is in meteorology/atmospheric physics, and has become more interested in the technology and tools that support our science."
  }
]